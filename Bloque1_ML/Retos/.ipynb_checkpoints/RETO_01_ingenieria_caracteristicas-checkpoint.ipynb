{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RETO: *Ingeniería de características*\n",
    "\n",
    "+ El objetivo de este reto es extraer o diseñar un mejor par de características para construir un clasificador linear mediante regresión logística.\n",
    "+ Ejecuta todo el cuaderno y después lee las instrucciones del reto, abajo del todo.\n",
    "---\n",
    "    [ES] Código de Alfredo Cuesta Infante para 'Reconocimiento de Patrones'\n",
    "       @ Master Universitario en Visión Artificial, 2020, URJC (España)\n",
    "    [EN] Code by Alfredo Cuesta-Infante for 'Pattern Recognition'\n",
    "       @ Master of Computer Vision, 2020, URJC (Spain)\n",
    "\n",
    "    alfredo.cuesta@urjc.es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PONED VUESTROS NOMBRES EN ESTA LISTA \n",
    "nombres = ['Alfredo Cuesta', 'Geoffrey Hinton'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------#\n",
    "# Extracción de características #\n",
    "#-------------------------------#\n",
    "def feat_extraction (data, theta=0.1):\n",
    "    # data: dataframe\n",
    "    # theta: parameter of the feature extraction\n",
    "    # features extracted: \n",
    "    #   'width','W_max1','W_max2','W_max3',\n",
    "    #   'height','H_max1','H_max2','H_max3',\n",
    "    #   'area','w_vs_h'\n",
    "    #\n",
    "    features = np.zeros([data.shape[0], 10]) #<- allocate memory with zeros\n",
    "    data = data.values.reshape([data.shape[0],28,28]) \n",
    "    #-> axis 0: id of instance, axis 1: width(cols) , axis 2: height(rows)\n",
    "    for k in range(data.shape[0]):\n",
    "        #..current image \n",
    "        x = data[k,:,:]\n",
    "        #--width feature\n",
    "        sum_cols = x.sum(axis=0) #<- axis0 of x, not of data!!\n",
    "        indc = np.argwhere(sum_cols > theta * sum_cols.max())\n",
    "        col_3maxs = np.argsort(sum_cols)[-3:] \n",
    "        features[k,0] = indc[-1] - indc[0]\n",
    "        features[k,1:4] = col_3maxs\n",
    "        #--height feature\n",
    "        sum_rows = x.sum(axis=1) #<- axis1 of x, not of data!!\n",
    "        indr = np.argwhere(sum_rows > theta * sum_rows.max())\n",
    "        features[k,4] = indr[-1] - indr[0]\n",
    "        row_3maxs = np.argsort(sum_rows)[-3:] \n",
    "        features[k,5:8] = row_3maxs\n",
    "    #--area\n",
    "    features[:,8] = features[:,0] * features[:,4]\n",
    "    #--ratio W/H\n",
    "    features[:,9] = features[:,0] / features[:,4]\n",
    "    col_names = ['width','W_max1','W_max2','W_max3','height','H_max1','H_max2','H_max3','area','w_vs_h']\n",
    "    #\n",
    "    return pd.DataFrame(features,columns = col_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------#\n",
    "# Separacion de datos #\n",
    "#---------------------#\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    train_set = data.iloc[train_indices]\n",
    "    test_set  = data.iloc[test_indices]\n",
    "    return train_set.reset_index(drop=True), test_set.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------#\n",
    "# Algunas funciones auxiliares #\n",
    "#------------------------------#\n",
    "\n",
    "def join_features_labels(X0,X1):\n",
    "    Y0 = pd.DataFrame(np.zeros(X0.shape[0]),columns=['label'])\n",
    "    XY0 = pd.concat([X0,Y0],axis=1)\n",
    "    Y1 = pd.DataFrame(np.ones(X1.shape[0]),columns=['label'])\n",
    "    XY1 = pd.concat([X1,Y1],axis=1)\n",
    "    return pd.concat([XY0,XY1],axis=0,ignore_index=True)\n",
    "\n",
    "def jitter(X,sigma=0.2):\n",
    "    random_sign = (-1)**np.random.randint(1,3,*X.shape)\n",
    "    return X + np.random.normal(0,sigma,*X.shape)*random_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../Datasets/1000ceros.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-42-8bbc876021da>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;31m# --- Get data -------------------------------------\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m \u001B[0mFullSet_0\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'../../Datasets/1000ceros.csv'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mheader\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m \u001B[0mFullSet_1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'../../Datasets/1000unos.csv'\u001B[0m\u001B[1;33m,\u001B[0m  \u001B[0mheader\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[0mFullSet_0\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mFullSet_0\u001B[0m \u001B[1;33m/\u001B[0m\u001B[1;36m255.\u001B[0m \u001B[1;31m#<- quick rescale to [0,1]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\myRP\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001B[0m\n\u001B[0;32m    684\u001B[0m     )\n\u001B[0;32m    685\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 686\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    687\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    688\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\myRP\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    450\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    451\u001B[0m     \u001B[1;31m# Create the parser.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 452\u001B[1;33m     \u001B[0mparser\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfp_or_buf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    453\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    454\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\myRP\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    934\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    935\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 936\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    937\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    938\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\myRP\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[1;34m(self, engine)\u001B[0m\n\u001B[0;32m   1166\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mengine\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"c\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1167\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mengine\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"c\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1168\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mCParserWrapper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1169\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1170\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mengine\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"python\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\myRP\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, src, **kwds)\u001B[0m\n\u001B[0;32m   1996\u001B[0m         \u001B[0mkwds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"usecols\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0musecols\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1997\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1998\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reader\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mparsers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTextReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1999\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munnamed_cols\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reader\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munnamed_cols\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2000\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader.__cinit__\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../../Datasets/1000ceros.csv'"
     ]
    }
   ],
   "source": [
    "#------------------------------#\n",
    "# Construcción de los datasets #\n",
    "#------------------------------#\n",
    "np.random.seed(seed=1234) #<- comment this to get randomness\n",
    "fraction_Test  = 0.2\n",
    "fraction_Valid = 0.2\n",
    "theta = 0.5\n",
    "\n",
    "# --- Get data -------------------------------------\n",
    "FullSet_0 = pd.read_csv('../../Datasets/1000ceros.csv', header=None)\n",
    "FullSet_1 = pd.read_csv('../../Datasets/1000unos.csv',  header=None)\n",
    "FullSet_0 = FullSet_0 /255. #<- quick rescale to [0,1]\n",
    "FullSet_1 = FullSet_1 /255. #<- quick rescale to [0,1]\n",
    "\n",
    "# --- Separate Test sets -----------------------------\n",
    "TrainSet_0, TestSet_0 = split_train_test(FullSet_0, fraction_Test)\n",
    "TrainSet_1, TestSet_1 = split_train_test(FullSet_1, fraction_Test)\n",
    "\n",
    "# --- Separate Validation sets -----------------------\n",
    "TrainSet_0, ValidSet_0 = split_train_test(TrainSet_0, fraction_Valid)\n",
    "TrainSet_1, ValidSet_1 = split_train_test(TrainSet_1, fraction_Valid)\n",
    "\n",
    "# --- Ensamble TRAIN SET, VALIDATION SET Y TEST SET --\n",
    "#          with the features and the labels\n",
    "Feat_train = join_features_labels(feat_extraction(TrainSet_0, theta=theta), \n",
    "                                  feat_extraction(TrainSet_1, theta=theta))\n",
    "Feat_valid = join_features_labels(feat_extraction(ValidSet_0, theta=theta),\n",
    "                                  feat_extraction(ValidSet_1, theta=theta))\n",
    "Feat_test  = join_features_labels(feat_extraction(TestSet_0,  theta=theta),\n",
    "                                  feat_extraction(TestSet_1,  theta=theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------#\n",
    "# Entrenamiento y evaluación #\n",
    "#----------------------------#\n",
    "\n",
    "#-[1].Select any 2 features from the list:\n",
    "#    -features list: 'width','W_max1','W_max2','W_max3','height','H_max1','H_max2','H_max3','area','w_vs_h'\n",
    "feat_1 = 'width'\n",
    "feat_2 = 'height'\n",
    "\n",
    "#-[2].Fit a LogisticRegression model (a linear classifier) with Feat_train dataframe\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(Feat_train[[feat_1, feat_2]], Feat_train['label'])\n",
    "\n",
    "#-[3].Predict the Feat_valid dataframe\n",
    "y_pred = clf.predict( Feat_valid[[feat_1, feat_2]] )\n",
    "\n",
    "#-[4].Compare the predictions with the ground truth\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_pred, Feat_valid['label'])\n",
    "N_success = conf_mat[0,0]+conf_mat[1,1]\n",
    "N_fails = conf_mat[0,1]+conf_mat[1,0]\n",
    "print (nombres,\"\\n\")\n",
    "print(\"Outcome:\")\n",
    "strlog = \"  :) HIT  = %d, (%0.2f%%)\"%(N_success, 100*N_success/(N_success+N_fails))\n",
    "print(strlog)\n",
    "strlog = \"  :( FAIL = %d, (%0.2f%%)\"%(N_fails, 100*N_fails/(N_success+N_fails))\n",
    "print(strlog)\n",
    "\n",
    "#-[5].Show the (linear) model parameters\n",
    "print(\"\\nLogistic regression model:\")\n",
    "print(\"  clf coef. = \",clf.coef_)\n",
    "print(\"  clf intercept = \",clf.intercept_)\n",
    "\n",
    "#-[6].Plot Feat_valid and the model\n",
    "ind = Feat_valid['label']==0\n",
    "x0, x1 = Feat_valid[ind][feat_1], Feat_valid[~ind][feat_1]\n",
    "y0, y1 = Feat_valid[ind][feat_2], Feat_valid[~ind][feat_2]\n",
    "plt.plot(jitter(x0),jitter(y0),'yo',jitter(x1),jitter(y1),'bx', alpha=.3)\n",
    "\n",
    "w = clf.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xmin=min(x0.min(axis=0),x1.min(axis=0))\n",
    "xmax=max(x0.max(axis=0),x1.max(axis=0))\n",
    "ymin=min(y0.min(axis=0),y1.min(axis=0))\n",
    "ymax=max(y0.max(axis=0),y1.max(axis=0))\n",
    "xx = np.linspace(xmin,xmax)\n",
    "yy = a * xx - (clf.intercept_[0] / w[1])\n",
    "plt.plot(xx,yy,'r')\n",
    "strTitle = \"w_X = %2.2f, w_Y = %2.2f, w_0 = %2.2f \" % (w[0], w[1], clf.intercept_[0])\n",
    "plt.axis([xmin-1,xmax+1,ymin-1,ymax+1])\n",
    "plt.title(strTitle)\n",
    "plt.xlabel(feat_1)\n",
    "plt.ylabel(feat_2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este reto consiste en extraer o diseñar dos características que mejor separen linealmente los ceros de los unos. \n",
    "Para ello:\n",
    "+ Podeis modificar la funcion **feat_extraction** y el parámetro **theta**\n",
    "+ NO podeis modificar el método de aprendizaje\n",
    "+ NO podeis utilizar más de dos características para aprender el modelo.\n",
    "+ NO podeis modificar la semilla de números pseudoaleatorios (seed=1234)\n",
    "\n",
    "Una vez lo hayais hecho, debeis ejecutar todo el cuaderno (*Kernel $\\rightarrow$ Restart & Run all* ) para obtener el resultado en la celda de arriba.\n",
    "\n",
    "Por último, teneis que:\n",
    "1. Descargar el cuaderno ( *File* $\\rightarrow$ *Download as* $\\rightarrow$ *Notebook* )<br>\n",
    "Hacer esto con el cuaderno **tal y como haya quedado al terminar la ejecución**\n",
    "2. Subirlo al aula virtual, en la pestaña *Evaluación*, en la tarea *Reto 1: Ingeniería de características*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-c49d2d5b",
   "language": "python",
   "display_name": "PyCharm (Repositorio_MUVA_RP_2020)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}